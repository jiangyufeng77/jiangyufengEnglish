\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx,bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage[pagebackref=true,breaklinks=true, letterpaper=true, colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\begin{document}
\title{Image-to-Image Translation with Conditional Adversarial Networks}
\author{Yufeng Jiang}
\maketitle

\section{Optimization and inference}

To optimize their networks, they follow the standard approach from~\cite{Generative}: they alternate between one gradient descent step on $D$, then one step on $G$. As suggested in the original GAN paper, rather than training $G$ to minimize $\log(1-D(x, G(x, z))$, they instead train to maximize $\log D(x, G(x, z))$~\cite{Generative}. In addition, they divide the objective by 2while optimizing $D$, which slows down the rate at which $D$ learns relative to $G$. They use minibatch SGD and apply the Adam slover, with learning rate 0.0002, and momentum parameters $\beta_1$ = 0.5, $\beta_2$ = 0.999.

At inference time, they run the generator net in exactly the same manner as during the training phase. This differs from the usual protocol in that they apply dropout at test time, and they apply batch normalization~\cite{batch} using the statistics of the test batch, rather than aggregated statistics of the training batch. This approach to batch normalization, when the batch size is set to 1, has been termed ``instance normalization'' and has been demonstrated to be effective at image generation tasks~\cite{Instance}. In their experiments, they use batch sizes between 1 and 10 depending on the experiment. 

\section{Experiments}

To explore the generality of conditional GANs, they test the method on a variety of tasks and datasets, including both graphics tasks, like photo generation, and vision tasks, like semantic segmentation.

\subsection{Evaluation metrics}

Evaluation the quality of synthesized images is an open and difficult problem. Traditional metrics such as perpixel mean-squared error do not assess joint statistics of the results, and therefore do not measure the very structure that structured losses aim to capture.

In order to more holistically evaluate the visual quality of their results, they employ two tactics. First, they run ``real vs fake'' perceptual studies on Amazon Mechanical Turk (AMT). For graphics problems like colorization and photo generation, plausibility to a human observer is often the ultimate goal. Therefore, they test their map generation, aerial photo generation, and image colorizaiton using this approach. 

Second, they measure whether or not their synthesized cityscapes are realistic enough that off-the-shelf recognition system can recognize the objects in them. This metric is similar to the ``inception score'' from~\cite{Improved}, the object detection evaluation in~\cite{Generative}, and the ``semantic interpretabiity'' measures in~\cite{Colorful}.

{\bf AMT perceptual studies} For their AMT experiments, they followed the protocol from~\cite{Colorful}: Turkers were presented with a series of trials that pitted a ``real'' image against a ``fake'' image generated by their algorithm. On each trial, each image appeared for 1 second, after which the images disappeared and Turkers were given unlimited time to respond as to which was fake. The first 10 iamges of each session were practice and Turkers were given feedback. No feedback was provided on the 40 trials of the main exeriment. Each session tested just one algorithm at a time, and Turkers were not allowed to complete more than one session.

\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{lccc}
  {\bf Loss} & {\bf Per-pixel acc.} & {\bf Per-class acc.} & {\bf Class IOU} \\
  \hline
  {\bf Encoder-decoder (L1)} & 0.35 & 0.12 & 0.08 \\
  {\bf Encoder-decoder (L1+cGAN)} & 0.29 & 0.09 & 0.05 \\
  {\bf U-net (L1)} & 0.48 & 0.18 & 0.13 \\
  {\bf U-net (L1+cGAN)} & {\bf 0.55} & {\bf 0.20} & {\bf 0.14}
\end{tabular}
\end{center}
\caption{FCN-scores for different generator architectures (and objectives), evaluated on Cityscapes labels$\leftrightarrow$photos. (U-net (L1-cGAN) scores differ from those reported in other tables since batch size was 10 for this experiment and 1 for other tables, and random variation between training runs.)}
\label{tab1}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{pix1.png}
\end{center}
\caption{Adding skip connections to an encoder-decoder to create a ``U-Net'' results in much higher quality results.}
\label{fig1}
\end{figure}

\begin{figure*}
\begin{center}
\includegraphics[width=16cm]{pix2.png}
\end{center}
\caption{Patch size variations. Uncertainty in the output manifests itself differently for different loss functions. Uncertain regions become blurry and desaturated under L1. The $1\times 1$ PixelGAN encourages greater color diversity but has no effect on spatial statistics. The $16\times 16$ PatchGAN creates locally sharp results, but also leads to tiling artifacts beyond the scale it can observe. The $70\times 70$ PatchGAN forces outputs that are sharp, even if incorrect, in both the spatial and spectral (colorfulness) dimensions. The full $286\times 286$ ImageGAN produces results that are visually similar to the  $70\times 70$ PatchGAN, but somewat lower quality according to their FCN-score metric (Table~\ref{tab2}). Please see {\color[gray]{0.6} https://phillipi.github.io/pix2pix/} for additional examples.}
\label{fig2}
\end{figure*}

{\bf ``FCN-score''} While quantitative evaluation of generative models is known to be challenging, recent works have tried using pre-trained semantic classifiers to measure the discriminability of the generated stimuli as a pseudo-metric. The intuition is that if the generatied images are realistic, classififers trained on real images will be able to classify the synthesized image correctlly as well. 

\subsection{Analysis of the generator architecture}

A U-Net architecture allows low-level information to shortcut across the network. Does this lead to better results? Figure~\ref{fig1} and Table~\ref{tab1} compare the U-Net against an encoder-decoder on cityscape generation. The encoder-decoder is created simply by severing the skip connections in the U-Net. The encoder-decoder is unable to learn to generate realistic images in their experiments. The advantages of the U-Net appear not to be specific to conditional GANs: when both U-Net and encoder-decoder are trained with an L1 loss, the U-Net again achieves the superior results.

\subsection{From PixelGANs to PatchGANs to ImageGANs}

They test the effect of varying the patch size $N$ of their discriminator receptive fields, from a $1 \times 1$ ``PixelGAN'' to a full $286 \times 286$ ``ImageGAN''\footnote{They achieve this variation in patch size by abjusting the depth of the GAN discriminator. Details of this process, and the discriminator architectures are provided in the supplemental materials online.}. Figure~\ref{fig2} shows qualitative results of this analysis and Table~\ref{tab2} quantifies the effects using the FCN-score. Note that elsewhere in this paper, unless specified, all experiments use $70\times 70$ PatchGANs, and for this section all experiments use an L1+cGAN loss.


\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{p{2cm}ccc}
  {\bf Discriminator receptive field} & {\bf Per-pixel acc.} & {\bf Per-class acc.} & {\bf Class IOU} \\
  \hline
  {\bf $1\times 1$} & 0.39 & 0.15 & 0.10 \\
  {\bf $16\times 16$} & 0.65 & 0.23 & {\bf 0.17} \\
  {\bf $70\times 70$} & {\bf 0.66} & {\bf 0.23} & {\bf 0.17} \\
  {\bf $286\times 286$} & 0.42 & 0.16 & 0.11
\end{tabular}
\end{center}
\caption{FCN-scores for different receptive field sizes of the discriminaotr, evaluated on Cityscapes lables$\rightarrow$photos. Notes that input images are $256\times 256$ pixels and larger rereptive fields are padded with zeros.}
\label{tab2}
\end{table}



{\small
\bibliographystyle{ieee}
\bibliography{pix2pix2}
}














\end{document}