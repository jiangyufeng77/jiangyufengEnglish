\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx,bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage[pagebackref=true,breaklinks=true, letterpaper=true, colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\begin{document}
\title{Image-to-Image Translation with Conditional Adversarial Networks}
\author{Yufeng Jiang}
\maketitle

\begin{abstract}

Authors investigate conditional adversrial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. They demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps and colorizing images.

\end{abstract}

\section{Introduction}

Many problems in image processing, computer graphics and computer vision can be posed as ``translating'' an input image into a corresponding output image. A scene may be renered as an RGB image, a gradient field, an edge map, a semanctic label map, etc. In analogy to automatic language translation, authors define automatic $image-to-image~translation$ as the task of translating one possible representation of a scene into another, given sufficient training data (see Figure~\ref{fig1}).

\begin{figure*}
\begin{center}
\includegraphics[width=16cm]{p1.png}
\end{center}
\caption{Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data.}
\label{fig1}
\end{figure*}

It would be highly desirable if they could instead specify only a high-level goal and then automatically learn a loss function appropriate for satisfying this goal. Fortunately, this is exactly what is done by the recnetly proposed Generative Adversarial Networks (GANs)~\cite{Generative,Deep,Unsupervised,Improved,Energy}. In this paper, they explore GANs in the conditional setting. Just as GANs learn a generative model of data, conditional GANs (CGANs) learn a conditional generative model~\cite{Generative}. This makes cGANs suitable for image-to-image translation tasks, where they condition on an input image and generate a corresponding output image.

\section{Method}

GANs are generative models that learn a mapping from random noise vector $z$ to output image $y$, $G : z \to y$~\cite{Generative}. In contrast, conditional GANs learn a mapping from observed image $x$ and random noise vector $z$, to $y$, $G : \{x, z\} \to y$. The generator $G$ is trained to produce outputs that cannot be distinguished from ``real'' images by an adversarially trained discriminator $D$, which is trained to do as well as possible at detecting the generator's ``fake''.

\subsection{Objective}

The objective of a conditional GAN can be expressed as\\
\begin{multline}
\mathcal{L}_{cGAN}(G, D) = \mathbb{E}_{x, y}[\log D(x, y)] \\
 + \mathbb{E}_{x, z}[\log(1 - D(x, G(x, z)))],
\end{multline} 
where $G$ tries to minimize this objective against an adversarial $D$ that tries to maximize it, \emph{i.e.} $G^{\ast} = arg\mathop{\min}\limits_{G}\mathop{\max}\limits_D\mathcal{L}_{cGAN}(G, D)$

To test the importance of conditioning the discriminator, they also compare to an unconditional variant in which the discriminator does not observe $x$:\\
\begin{multline}
\mathcal{L}_{cGAN}(G, D) = \mathbb{E}_y[\log D(y)] \\
+ \mathbb{E}_{x, z}[\log(1 - D(G(x, z)))].
\end{multline}
Previous approaches have found it beneficial to mix the GAN objective with a more traditional loss, such as L2 distance~\cite{Context}. The discriminator's job remains unchanges, but the generator is tasked to not only fool the discriimator but also to be near the ground truth output in an L2 sense. They also explore this option, using L1 distance rather than L2 as L1 encourages less blurring:\\
\begin{gather}
\mathcal{L}_{L1}(G) = \mathbb{E}_{x, y, z}[||y - G(x, z)||_1].
\end{gather}
Their final objective is \\
\begin{gather}
G^{\ast} = arg\mathop{\min}\limits_G \mathop{\max}\limits_D\mathcal{L}_{cGAN}(G, D) + \lambda\mathcal{L}_{L1}(G).
\end{gather}

\subsection{Network architectures}

They adapt their generator and discriminator architectures from those in~\cite{Unsupervised}. Both generaotr and discriminator use modules of the form convolution-BatchNorm-ReLU. Details of the architecture are provided in the supplemental materials online, with key features discussed below. 

{\bf Generator with skips:} A defining feature of image-to-image translation problems is that they map a high resolution input grid ot a high resolution output grid. To give the generator a means to circumvent the bottleneck for information like this, they add skip connections, following the general shape of a ``U-Net'' (see Figure \ref{fig2}). Specifically, they add skip connections between each layer $i$ and layer $n_i$, where $n$ is the total number of layers. Each skip connection simply concatenates all channels at layer $i$ with those at layer $n_i$. The evaluation of the generator architecture is shown at Table~\ref{tab1}

\begin{figure}[hb]
\begin{center}
\includegraphics[width=8cm]{p2.png}
\end{center}
\caption{Two choices for the architecture of the generator. The ``U-Net'' is an encoder-decoder with skip connections between mirrored layers in the encoder and decoder stacks.}
\label{fig2}
\end{figure}

\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{p{3.5cm}p{1.2cm}p{1.2cm}p{1cm}}
  {\bf Loss} & {\bf Per-pixel acc.} & {\bf Per-class acc.} & {\bf Class IOU} \\ 
  \hline
  {\bf Encoder-decoder (L1)} & 0.35 & 0.12 & 0.08 \\
  {\bf Encoder-decoder (L1 + cGAN)} & 0.29 & 0.09 & 0.05 \\
  {\bf U-net (L1)} & 0.48 & 0.18 & 0.13 \\
  {\bf U-net (L1 + cGAN)} & {\bf 0.55} & {\bf 0.20} & {\bf 0.14}\\
\end{tabular}
\end{center}
\caption{FCN-scores for different generator architectures (and objectives), evaluated on Cityscapes labelsâ†”photos. (U-net (L1-cGAN) scores differ from those reported in other tables since batch size was 10 for this experiment and 1 for other tables, and random variation between training runs.)}
\label{tab1}
\end{table}

{\bf Markovian discriminator {PatchGAN}:} It is well known that the L2 loss - and L1 prodidcues blurry results on image generation problems. GAN discriminaotr only models high-frequency structure, relying on an L1 term to force low-frequency correctness. In order to model high-frequencies, it is sufficient to restrict their attention to the structure in local image patches. Thesefore, they design a discriminator architecture which they term a $PatchGAN$ that only penalizes structure at the scale of patches. This discriminator tries to classify if each $N\times N$ patch in an image is real or fake. They run this discriminator convolutationally across the iamge, averaging all responses to provide the ultimate output of $D$.


{\small
\bibliographystyle{ieee}
\bibliography{pix2pix}
}












\end{document}