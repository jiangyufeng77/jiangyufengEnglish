\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage[pagebackref=true,breaklinks=true, letterpaper=true, colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy
\def\cvprPaperID{****}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\begin{document}
\title{Multipath Sparse Coding Using Hierarchical Matching Pursuit}
\author{Yufeng Jiang}
\maketitle

\section{Hierarchial Matching Pursuit}

In their hierarchical matching pursuit, MI-KSVD is used to learn codebooks at three layers, where the data matrix $Y$ in the first layer consists of raw patches sampled from images, and $Y$ in the second and third layers are sparse codes pooled from the lower layers. With the learn codebooks $D$, hierarchical matching pursuit builds a feature hierarchy, layer by layer, using batch orthogonal matching pursuit for computing sparse codes,  spatial pooling for aggregating sparse codes, and contrast normalization for normalizing feature vectors, as shown in Fig.~\ref{fig1} used in~\cite{multipath}
{\bf First Layer:} The goal of the layer in HMP is to extract sparse codes for small patches (\emph{e.g.} 5$\times$5) and generate pooled codes for mid-level patches (\emph{e.g.} 16$\times$16). Orthogonal matching pursuit is used to computed the sparse codes $x$ of small pathces (\emph{e.g.} 5$\times$5 pixels) The features of each spatial cell $C$ are the max pooled sparse codes, which are simple the component-wise maxima over all sparse codes within a cell~\cite{multipath}:
\begin{gather}
F(C) = \mathop{\max}\limits_{j\in C}[max(x_{j1},0),\dots,max(x_{jM},0),\dots,\\ max(-x_{j1},0),\dots,max(-x_{jM},0)]  \notag
\end{gather}
Here, $j$ ranges over all entries in the cell, and $x_{jm}$ is the $m$-th component of the sparse code vector $x_j$ of entry $j$. Authors split the positive and negative components of the sparse codes into separae features to allow higher layers weight positive and negative reponses differently. The feature $F_P$ describing an image patch $P$ is the concatenation of aggregated sparse codes in each spatial cell~\cite{multipath}
\begin{gather}
F_P = [F(C_1^P),\dots,F(C_s^P),\dots,F(C_S^P)]
\end{gather}
where $C_s^P \subseteq P$ is a spatial cell geneated by spatial partitions, and $S$ is the total number of spatial cells. They additionally normalize the feature vectors $F_P$ by $L_2$ norm $\sqrt{||F_P||^2 + \varepsilon}$, where $\varepsilon$ is a small positive number. Since the magitude of sparse codes varies over a wide range due to local variations in illumination and occlusion, this operation makes the appearance features robust to such variations.\\
\begin{figure}
\begin{center}
\includegraphics[width=8cm]{m31.png}
\end{center}
\caption{A three-layer architecture of Hierarchical Matching Pursuit.}
\label{fig1}
\end{figure}
{\bf Second Layer:} The goal of the second layer in HMP is to gather and code mid-level sparse codes and generate pooled codes for large patches (\emph{e.g.} 36$\times$36). HMP applies batch OMP and spatial max pooling to features $F_P$ generated in the first layer.\\
{\bf Third Layer:} The goal of the third layer in HMP is to generate pooled sparse codes for the whole image. Similar to the second layer, the codebook for this level is learned by sampling these pooled sparse codes in the second layer.

\section{Experiments}

Authors evaluate the proposed M-HMP models on five standard vision datasets on object, scene and fine-grained recognition, extensively comparing to state-of-the-art algorithms using designed and learned features. All images are resized to 300 pixels on the longest side.

\balance

\subsection{Object Recognition}\label{2.1}

\begin{figure}
\begin{center}
\includegraphics[width=8cm]{m32.png}
\end{center}
\caption{Test accuracy of single-path and multipath HMP. A1,A2,B1,B2 and B3 denotes the HMP networks of different architectures. A1+A2 indicates the combination of A1 and A2. ``All'' means the combination of all five paths: A1,A2,B1,B2 and B3.}
\label{fig2}
\end{figure}

They investigate the behavior of M-HMP for object category recognition on Caltech-101. The results of M-HMP are shown in Fig.~\ref{fig2} cited in~\cite{multipath}. As can been seen, the one-layer HMP networks (A1 and B1) work surprisingly well and already outperform many existing computer vision approaches, showing the benefits of learning from pixels.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  SIFT+T~\cite{importance} & 67.7 & HSC~\cite{image} & 74.0 \\ \hline
  Local NBNN~\cite{Local} & 71.9 & Asklocals~\cite{Ask} & 77.1 \\ \hline
  LC-KSVD~\cite{Learning} & 73.6 & LP-$\beta$~\cite{On} & 77.7 \\ \hline
  LLC~\cite{constrained} & 73.4 & FK~\cite{The} & 77.8 \\ \hline
  HMP~\cite{Hierarchical} & 76.8 & M-HMP & {\bf 82.5}+{\bf 0.5} \\
  \hline
\end{tabular}
\end{center}
\caption{Test accuracy on Caltech-101.}
\label{tab1}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
  \hline
  Training Images & 15 & 30 & 45 & 60 \\ \hline
  Local NBNN~\cite{Local} & 33.5 & 40.1 & / & / \\ \hline
  LLC~\cite{constrained} & 34.4 & 41.2 & 45.3 & 47.7 \\ \hline
  CRBM~\cite{Efficient} & 35.1 & 42.1 & 45.7 & 47.9 \\ \hline
  LP-$\beta$~\cite{On} & / & 45.8 & / & / \\ \hline
  M-HMP & {\bf 42.7} & {\bf 50.7} & {\bf 54.8} & {\bf 58.0} \\ 
  \hline
\end{tabular}
\end{center}
\caption{Test accuracy on Caltech-256.}
\label{tab2}
\end{table}


They compare M-HMP with recently published state-of-the-art recognition algorithms in Tab.~\ref{tab1}~\cite{multipath}. LLC~\cite{constrained}, LC-KSVD~\cite{Learning}, and Asklocals~\cite{Ask} are one-layer sparse coding approaches. SIFT+T~\cite{importance} is soft threshold coding and CRBM~\cite{Efficient} is a convolutional variant of Restricted Boltzmann Machines (RBM). FK~\cite{The} is a Fisher Kernel based coding approach. All of them are based on SIFT. HSC~\cite{image} is a two layer sparse coding network using L1-norm regularization. Local NBNN~\cite{Local} is an extension of Naive Bayesian Nearest Neighbor (NBNN). LP-$\beta$~\cite{On} is a boosting approach to combine multiple types of designed features. M-HMP achieves test accuracy superior to all of them. The average accuracy over 5 random trials in Tab.~\ref{tab2}~\cite{multipath}. They keep the same architecture as that for Caltech-101 (Section~\ref{2.1}), with the only exception that the number of codewords in the final layer of HMP is increased to 2000 to accommodate for more categories and more images.

{\small
\bibliographystyle{ieee}
\bibliography{multipath3}
}












\end{document}