\documentclass[10pt,letterpaper]{article}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx,float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{indentfirst}
\usepackage{balance,multicol}
\usepackage{cite}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{geometry}
\geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}
\setlength{\parindent}{2em}
\title{Densely Connected Convolutional Networks}
\author{Yufeng Jiang}
\date{\today}
\begin{document}
\maketitle
\balance
\begin{multicols}{2}
\section{DenseNets}
{\bf Pooling layers.} The down-sampling layers change the size of feature-maps and are essential for convolutional networks. Authors divide the networks into multiple densely connected dense blocks to facilitate down-sampling in the architecture just as shown at Figure \ref{fig}. \\
{\bf Growth rate.} If each function $H_\ell$ produces $\kappa$ feature-maps, it follows that the $\ell^{th}$ layer has $\kappa_0 + \kappa \times (\ell-1)$ input feature-maps. $\kappa_0$ is the number of channels in the input layer. One explanation for this is that each layer has access to all the preceding feature-maps in its block. Each layer adds $\kappa$ feature-maps of its own to this state.\\
\end{multicols}
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=16cm]{1.png}
\end{center}
\caption{A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change feature-map sizes via convolution and pooling.}
\label{fig}
\end{figure}
\begin{table}[H]
\begin{center}
\begin{tabular}{c|c|c|c|c|c}
  \hline
  Layers & Output Size & DenseNet-121 & DenseNet-169 & DenseNet-201 & DenseNet-264 \\ \hline
  Convolution & 112 $\times$ 112 & \multicolumn{4}{|c}{7 $\times$ 7 conv, stride 2} \\ \hline
  Pooling & 56 $\times$ 56 & \multicolumn{4}{|c}{3 $\times$ 3 max pool, stride 2} \\ \hline
  Dense Block (1) & 56 $\times$ 56 & 
  $
  \left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 6 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 6 
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 6 
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 6 
 $ \\ \hline
\multirow{2}{*}{Transition Layer (1)} & 56 $\times$ 56 & \multicolumn{4}{|c}{1 $\times$ 1 conv} \\ \cline{2-6}
& 28 $\times$ 28 & \multicolumn{4}{|c}{2 $\times$ 2 averge pool, stride 2}\\ 
\hline
  Dense Block (2) & 28 $\times$ 28 & 
  $
  \left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 12 
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 12 
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 12
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 12 
 $ \\ \hline
 \multirow{2}{*}{Transition Layer (2)} & 28 $\times$ 28 & \multicolumn{4}{|c}{1 $\times$ 1 conv} \\ \cline{2-6}
& 14 $\times$ 14 & \multicolumn{4}{|c}{2 $\times$ 2 averge pool, stride 2}\\ 
\hline
  Dense Block (3) & 28 $\times$ 28 & 
  $
  \left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 24 
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 32
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 48
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 64 
 $ \\ \hline
 \multirow{2}{*}{Transition Layer (3)} & 14 $\times$ 14 & \multicolumn{4}{|c}{1 $\times$ 1 conv} \\ \cline{2-6}
& 7 $\times$ 7 & \multicolumn{4}{|c}{2 $\times$ 2 averge pool, stride 2}\\ 
\hline
  Dense Block (4) & 7 $\times$ 7 & 
  $
  \left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 16
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 32
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 32
 $ &
 $
\left[
  \begin{matrix}
    1 \times 1~conv \\
    3 \times 3~conv \\
  \end{matrix}
  \right]
  \times 48 
 $ \\ \hline
\multirow{2}{*}{Classification Layer} & 1 $\times$ 1 & \multicolumn{4}{|c}{7 $\times$ 7 global average pool} \\ \cline{2-6}
& & \multicolumn{4}{|c}{2 $\times$ 1000D fully-connected, softmax}\\ 
\hline
\end{tabular}
\caption{DenseNet architectures for ImageNet. The growth rate for all the networks is $\kappa$ = 32. Note that each conv layer shown in the table corresponds the sequence BN-ReLU-Conv.}
\label{tab}
\end{center}
\end{table}
\begin{multicols}{2}
{\bf Bottleneck layers.} Although each layer only produces $\kappa$ output feature-maps, it typically has many more inputs. It has been noted in~\cite{Deep,Training} that a 1 $\times$ 1 convolution can be introduced as bottleneck layer before each 3 $\times$ 3 convolution to reduce the number of input feature-maps. \\
{\bf Compression} To further improve model cpmpactness, authors reduce the number of feature-maps at transition layers. Authors let the following transition layer generate $\theta_m$ output feature-maps, where 0 $< \theta \leq $ 1 is referred to as the compression factor. In this experiments on ImageNet, author use a DenseNet-BC structure with 4 dense blocks on 224 $\times$ 224 input images. The exact network configureations are shown in Table\ref{tab}.
{\small
\bibliographystyle{IEEEtranS}
\bibliography{cc}	
}
\end{multicols}
\end{document}



